Test:
    model_name: stub_model
    target: debater
    max_length: 4096
    requires_token: True
    opening_speeches_only: False
    llm_type: stub_llm
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_8bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: stub_model
    dataset:
        dataset_type: quality_debates
Train - 13B - Alpaca:
    model_name: Yukang/LongAlpaca-13B
    target: debater
    opening_speeches_only: False
    llm_type: llama
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K-Neft-4/
        merge_output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K-Merged-Neft-4/
    dataset:
        dataset_type: quality_debates
Train - Mixtral:
    model_name: /vast/spa9663/models/base_models/mixtral-8x7b
    llm_type: mistral
    target: debater
    opening_speeches_only: False
    max_length: 32986
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b
        merge_output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b-merged
    dataset:
        dataset_type: quality_debates
Train - Mixtral - Unified:
    model_name: /vast/spa9663/models/base_models/mixtral-8x7b
    llm_type: mistral
    target: debater
    opening_speeches_only: False
    max_length: 32986
    training_hyperparameters:
        num_train_epochs: 3
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/mixtral-mega-adapter
        merge_output_dir: /vast/spa9663/models/trained_models/mixtral-mega
    dataset:
        dataset_type: quality_debates
        full_dataset_file_path: /home/spa9663/debate-data/mega_human_and_gpt4_debates.jsonl
Quote Relevance - Mixtral:
    model_name: /vast/spa9663/models/base_models/mixtral-8x7b
    llm_type: mistral
    target: debater
    opening_speeches_only: False
    max_length: 32986
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b-scratchpad
        merge_output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b-scratchpad-merged
    dataset:
        dataset_type: quote_relevance
    scratchpad_config:
        use_scratchpad: True