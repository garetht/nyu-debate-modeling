Default - Local:
    model_name: meta-llama/Llama-2-13b-hf
    target: debater
    max_length: 4096
    requires_token: True
    opening_speeches_only: False
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_8bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-Memorized-4/
    prompt_config:
        use_scratchpad: True
    dataset:
        dataset_type: quote_relevance
Train - 13B - Alpaca:
    model_name: Yukang/LongAlpaca-13B
    target: debater
    opening_speeches_only: False
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K-Neft-4/
        merge_output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K-Merged-Neft-4/
    dataset:
        dataset_type: quality_debates
Train - Mixtral:
    model_name: /vast/spa9663/models/base_models/mixtral-8x7b
    llm_type: mistral
    target: debater
    opening_speeches_only: False
    max_length: 32986
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b
        merge_output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b-merged
    dataset:
        dataset_type: quality_debates
Quote Relevance - Mixtral:
    model_name: /vast/spa9663/models/base_models/mixtral-8x7b
    llm_type: mistral
    target: debater
    opening_speeches_only: False
    max_length: 32986
    training_hyperparameters:
        num_train_epochs: 4
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 4
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
        peft_type: lora
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b-scratchpad
        merge_output_dir: /vast/spa9663/models/trained_models/mixtral-8x7b-scratchpad-merged
    prompt_config:
        use_scratchpad: True
    dataset:
        dataset_type: quote_relevance