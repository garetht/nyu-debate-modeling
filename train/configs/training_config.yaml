Default - Local:
    model_name: Yukang/Llama-2-13b-longlora-32k-ft
    target: debater
    training_hyperparameters:
        num_train_epochs: 1
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /Users/samuelarnesen/debate-data/trained_models
    prompt_config:
        prompts_file_path: ./agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Default - HPC:
    model_name: /vast/spa9663/models/base_models/Llama-2-7b-chat-hf/
    target: debater
    training_hyperparameters:
        num_train_epochs: 1
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-7b-chat-hf/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Extended - 7B:
    model_name: /vast/spa9663/models/base_models/Llama-2-7B-32K-Instruct-Quantized/
    target: debater
    training_hyperparameters:
        num_train_epochs: 3
        per_device_train_batch_size: 1
        gradient_accumulation_steps: 8
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-7B-32K-Instruct/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
    deepspeed: /scratch/spa9663/env0/deepspeed_config.json
Extended - 7B - NoDS:
    model_name: /vast/spa9663/models/base_models/Llama-2-7B-32K-Instruct-Quantized/
    target: debater
    training_hyperparameters:
        num_train_epochs: 1
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-7B-32K-Instruct/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Remote - 7B:
    model_name: togethercomputer/Llama-2-7B-32K-Instruct
    target: debater
    training_hyperparameters:
        num_train_epochs: 3
        per_device_train_batch_size: 1
        gradient_accumulation_steps: 8
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-7B-32K-Instruct/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Quantize - 7B:
    model_name: /vast/spa9663/models/base_models/Llama-2-7B-32K-Instruct/
    target: debater
    training_hyperparameters:
        num_train_epochs: 3
        per_device_train_batch_size: 1
        gradient_accumulation_steps: 8
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/base_models/Llama-2-7B-32K-Instruct-Quantized/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Quantize - 13B:
    model_name: Yukang/Llama-2-13b-longlora-32k-ft
    target: debater
    training_hyperparameters:
        num_train_epochs: 16
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Train - 13B:
    model_name: /vast/spa9663/models/base_models/Llama-2-13B-32K/
    target: debater
    training_hyperparameters:
        num_train_epochs: 2
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Train - 13B - Alpaca:
    model_name: Yukang/LongAlpaca-13B
    target: debater
    training_hyperparameters:
        num_train_epochs: 2
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Train - Full - 13B:
    model_name: /vast/spa9663/models/base_models/Llama-2-13B-32K/
    target: debater
    training_hyperparameters:
        num_train_epochs: 1
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K-Full/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt
Train - Full - 13B - Judge:
    model_name: /vast/spa9663/models/base_models/Llama-2-13B-32K/
    target: judge
    training_hyperparameters:
        num_train_epochs: 1
        per_device_train_batch_size: 4
        gradient_accumulation_steps: 2
        optim: paged_adamw_32bit
        learning_rate: 2e-4
        max_grad_norm: 0.3
        warmup_ratio: 0.03
        lr_scheduler_type: constant
    logging_and_saving_config:
        logging_steps: 10
        output_dir: /vast/spa9663/models/trained_models/Llama-2-13B-32K-Full-Judge/
    prompt_config:
        prompts_file_path: /home/spa9663/debate/agents/prompts/prompts.yaml
        prompt_name: Base Prompt